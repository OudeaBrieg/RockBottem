{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8103ce09",
   "metadata": {},
   "source": [
    "# Challenge 1 : Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f58ed7",
   "metadata": {},
   "source": [
    "## Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4b0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Reinforcement Learning\n",
    "import gym\n",
    "\n",
    "# Distributed Learning\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Display\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from IPython.display import Video\n",
    "\n",
    "# Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0747a1e",
   "metadata": {},
   "source": [
    "## Context :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b1204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4006140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62de9aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00986755,  0.02455244,  0.03689659,  0.00625606], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32789dd3",
   "metadata": {},
   "source": [
    "## Random Action :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0714fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_video_folder_sanity(path, video_name):\n",
    "    video_path = path + video_name\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    if os.path.exists(video_path + \".mp4\"):\n",
    "        os.remove(video_path + \".mp4\")\n",
    "    if os.path.exists(video_path + \".meta.json\"):\n",
    "        os.remove(video_path + \".meta.json\")\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e277230",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = \"before_training\"\n",
    "path = \"videos/cart_pole/\"\n",
    "random_seed = 42\n",
    "\n",
    "video_path = check_video_folder_sanity(path, video_name)\n",
    "    \n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.action_space.seed(random_seed)\n",
    "before_video = VideoRecorder(env, video_path + \".mp4\", enabled=video_name is not None)\n",
    "\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "    env.render()\n",
    "    before_video.capture_frame()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    #print(\"step\", i, observation, reward, done, info)\n",
    "    \n",
    "before_video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4812eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/cart_pole/before_training.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_path + \".mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dc39d",
   "metadata": {},
   "source": [
    "## Train an agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c806b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "161b57ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 22.49425287356322\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 60.0\n",
      "  episode_reward_mean: 22.49425287356322\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_total: 174\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.68302429568383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01001952803287165\n",
      "          policy_loss: -0.011653771173329122\n",
      "          total_loss: 253.44734116420952\n",
      "          vf_explained_var:\n",
      "          - -0.0007393888081423938\n",
      "          vf_loss: 253.45699172481415\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.883333333333333\n",
      "    ram_util_percent: 66.55000000000001\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03471432895078949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.039400010630823504\n",
      "    mean_inference_ms: 0.6407755485241082\n",
      "    mean_raw_obs_processing_ms: 0.05562284003490809\n",
      "  time_since_restore: 3.834876298904419\n",
      "  time_this_iter_s: 3.834876298904419\n",
      "  time_total_s: 3.834876298904419\n",
      "  timers:\n",
      "    learn_throughput: 1784.944\n",
      "    learn_time_ms: 2240.967\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2523.47\n",
      "    sample_time_ms: 1585.119\n",
      "    update_time_ms: 1.984\n",
      "  timestamp: 1658599282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.83488</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.4943</td><td style=\"text-align: right;\">                  60</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.4943</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-01-30\n",
      "  done: false\n",
      "  episode_len_mean: 40.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 147.0\n",
      "  episode_reward_mean: 40.39\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 413\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.20000000000000004\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.6355716730958672\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005707622491848895\n",
      "          policy_loss: -0.016026466732384056\n",
      "          total_loss: 653.9133229573567\n",
      "          vf_explained_var:\n",
      "          - -0.0005345064564608037\n",
      "          vf_loss: 653.9282075779413\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.583333333333332\n",
      "    ram_util_percent: 66.56666666666666\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032699183771479026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04083111657754608\n",
      "    mean_inference_ms: 0.6443504293685693\n",
      "    mean_raw_obs_processing_ms: 0.05272896119187551\n",
      "  time_since_restore: 11.560508728027344\n",
      "  time_this_iter_s: 3.816021203994751\n",
      "  time_total_s: 11.560508728027344\n",
      "  timers:\n",
      "    learn_throughput: 1763.933\n",
      "    learn_time_ms: 2267.66\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2534.107\n",
      "    sample_time_ms: 1578.465\n",
      "    update_time_ms: 1.819\n",
      "  timestamp: 1658599290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         11.5605</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   40.39</td><td style=\"text-align: right;\">                 147</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             40.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-01-38\n",
      "  done: false\n",
      "  episode_len_mean: 78.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.0\n",
      "  episode_reward_mean: 78.86\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 513\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.10000000000000002\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5948357629519637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025950966748144815\n",
      "          policy_loss: -0.011967614183943438\n",
      "          total_loss: 1423.2395595099335\n",
      "          vf_explained_var:\n",
      "          - 0.0007547324057668447\n",
      "          vf_loss: 1423.2512715329406\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.783333333333331\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03302601550363926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04126945409562686\n",
      "    mean_inference_ms: 0.6434841191664716\n",
      "    mean_raw_obs_processing_ms: 0.05128858748922404\n",
      "  time_since_restore: 19.164771556854248\n",
      "  time_this_iter_s: 3.7403151988983154\n",
      "  time_total_s: 19.164771556854248\n",
      "  timers:\n",
      "    learn_throughput: 1771.205\n",
      "    learn_time_ms: 2258.349\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2551.527\n",
      "    sample_time_ms: 1567.689\n",
      "    update_time_ms: 1.785\n",
      "  timestamp: 1658599298\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         19.1648</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   78.86</td><td style=\"text-align: right;\">                 257</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             78.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-01-45\n",
      "  done: false\n",
      "  episode_len_mean: 130.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 359.0\n",
      "  episode_reward_mean: 130.05\n",
      "  episode_reward_min: 16.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 558\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.025000000000000005\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5895098480486101\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003539758422457492\n",
      "          policy_loss: -0.011443574891816223\n",
      "          total_loss: 2005.898864155431\n",
      "          vf_explained_var:\n",
      "          - -0.04575718939304352\n",
      "          vf_loss: 2005.9102147256174\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.14\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032837211132284175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04128427895724883\n",
      "    mean_inference_ms: 0.6399839523953027\n",
      "    mean_raw_obs_processing_ms: 0.049974849713747424\n",
      "  time_since_restore: 26.646183252334595\n",
      "  time_this_iter_s: 3.7411410808563232\n",
      "  time_total_s: 26.646183252334595\n",
      "  timers:\n",
      "    learn_throughput: 1780.632\n",
      "    learn_time_ms: 2246.394\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2574.528\n",
      "    sample_time_ms: 1553.683\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1658599305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         26.6462</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  130.05</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            130.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 174.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 410.0\n",
      "  episode_reward_mean: 174.75\n",
      "  episode_reward_min: 21.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 598\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.006250000000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5856178797701354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005694413540360578\n",
      "          policy_loss: -0.010125574309338806\n",
      "          total_loss: 2034.7880684801328\n",
      "          vf_explained_var:\n",
      "          - -0.01390852965414524\n",
      "          vf_loss: 2034.7981919155327\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.16\n",
      "    ram_util_percent: 66.5\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03259847387895564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04106776981662772\n",
      "    mean_inference_ms: 0.6369270176925336\n",
      "    mean_raw_obs_processing_ms: 0.049019877409714877\n",
      "  time_since_restore: 34.21847462654114\n",
      "  time_this_iter_s: 3.7415378093719482\n",
      "  time_total_s: 34.21847462654114\n",
      "  timers:\n",
      "    learn_throughput: 1778.251\n",
      "    learn_time_ms: 2249.401\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2586.743\n",
      "    sample_time_ms: 1546.346\n",
      "    update_time_ms: 1.543\n",
      "  timestamp: 1658599313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         34.2185</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  174.75</td><td style=\"text-align: right;\">                 410</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            174.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-01\n",
      "  done: false\n",
      "  episode_len_mean: 210.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 210.69\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 627\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0015625000000000003\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.567845380754881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0013675496000399015\n",
      "          policy_loss: -0.011337515876017591\n",
      "          total_loss: 2306.22373210948\n",
      "          vf_explained_var:\n",
      "          - 0.022543713450431824\n",
      "          vf_loss: 2306.2350606938844\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.36\n",
      "    ram_util_percent: 66.47999999999999\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03269868306882513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04069278836213867\n",
      "    mean_inference_ms: 0.6350509818802224\n",
      "    mean_raw_obs_processing_ms: 0.04831941573921292\n",
      "  time_since_restore: 41.759843587875366\n",
      "  time_this_iter_s: 3.7829341888427734\n",
      "  time_total_s: 41.759843587875366\n",
      "  timers:\n",
      "    learn_throughput: 1779.167\n",
      "    learn_time_ms: 2248.244\n",
      "    load_throughput: 77852510.441\n",
      "    load_time_ms: 0.051\n",
      "    sample_throughput: 2600.361\n",
      "    sample_time_ms: 1538.248\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1658599321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         41.7598</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  210.69</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  41</td><td style=\"text-align: right;\">            210.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 251.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 251.27\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 653\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0003906250000000001\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.572436648158617\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038534951624086947\n",
      "          policy_loss: -0.010867985549272709\n",
      "          total_loss: 1767.7551137288413\n",
      "          vf_explained_var:\n",
      "          - -0.012719376012682915\n",
      "          vf_loss: 1767.7659789054624\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.616666666666664\n",
      "    ram_util_percent: 66.51666666666667\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03266309980388879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04052824367727807\n",
      "    mean_inference_ms: 0.6339544774505934\n",
      "    mean_raw_obs_processing_ms: 0.047743052026632954\n",
      "  time_since_restore: 49.35251045227051\n",
      "  time_this_iter_s: 3.806361675262451\n",
      "  time_total_s: 49.35251045227051\n",
      "  timers:\n",
      "    learn_throughput: 1781.482\n",
      "    learn_time_ms: 2245.321\n",
      "    load_throughput: 77852510.441\n",
      "    load_time_ms: 0.051\n",
      "    sample_throughput: 2618.076\n",
      "    sample_time_ms: 1527.84\n",
      "    update_time_ms: 1.392\n",
      "  timestamp: 1658599328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         49.3525</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  251.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  41</td><td style=\"text-align: right;\">            251.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 282.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 282.83\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 679\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.765625000000002e-05\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5490572565345354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034496347788371953\n",
      "          policy_loss: -0.013337810403637348\n",
      "          total_loss: 1920.6875665808236\n",
      "          vf_explained_var:\n",
      "          - -0.03204629197716713\n",
      "          vf_loss: 1920.700900498257\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.116666666666667\n",
      "    ram_util_percent: 66.46666666666665\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03269955391951736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.040450563895005225\n",
      "    mean_inference_ms: 0.6335215571043952\n",
      "    mean_raw_obs_processing_ms: 0.047213239192675754\n",
      "  time_since_restore: 57.1474289894104\n",
      "  time_this_iter_s: 3.873141288757324\n",
      "  time_total_s: 57.1474289894104\n",
      "  timers:\n",
      "    learn_throughput: 1770.136\n",
      "    learn_time_ms: 2259.713\n",
      "    load_throughput: 77852510.441\n",
      "    load_time_ms: 0.051\n",
      "    sample_throughput: 2609.816\n",
      "    sample_time_ms: 1532.675\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1658599336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         57.1474</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  282.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            282.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-24\n",
      "  done: false\n",
      "  episode_len_mean: 308.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 308.4\n",
      "  episode_reward_min: 117.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 703\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4414062500000005e-05\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5534907524944633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024791394974940715\n",
      "          policy_loss: -0.01072845068930458\n",
      "          total_loss: 1446.6880124656102\n",
      "          vf_explained_var:\n",
      "          - -0.02498968318104744\n",
      "          vf_loss: 1446.6987338814683\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.283333333333335\n",
      "    ram_util_percent: 66.41666666666667\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032666960560562223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.040439765238214784\n",
      "    mean_inference_ms: 0.6337141507224368\n",
      "    mean_raw_obs_processing_ms: 0.04683858993995997\n",
      "  time_since_restore: 64.84073781967163\n",
      "  time_this_iter_s: 3.8906476497650146\n",
      "  time_total_s: 64.84073781967163\n",
      "  timers:\n",
      "    learn_throughput: 1757.47\n",
      "    learn_time_ms: 2275.999\n",
      "    load_throughput: 39624978.744\n",
      "    load_time_ms: 0.101\n",
      "    sample_throughput: 2601.698\n",
      "    sample_time_ms: 1537.457\n",
      "    update_time_ms: 1.438\n",
      "  timestamp: 1658599344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         64.8407</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   308.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 117</td><td style=\"text-align: right;\">             308.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 337.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 337.48\n",
      "  episode_reward_min: 117.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 722\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.103515625000001e-06\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5295290557927983\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011084930878813989\n",
      "          policy_loss: -0.009904616856847398\n",
      "          total_loss: 1521.6032169465095\n",
      "          vf_explained_var:\n",
      "          - -0.04113409295678139\n",
      "          vf_loss: 1521.6131323701593\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.0\n",
      "    ram_util_percent: 66.39999999999999\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032625035345781324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04055540702206967\n",
      "    mean_inference_ms: 0.6341043339224545\n",
      "    mean_raw_obs_processing_ms: 0.04664484545738126\n",
      "  time_since_restore: 72.52992177009583\n",
      "  time_this_iter_s: 3.8202080726623535\n",
      "  time_total_s: 72.52992177009583\n",
      "  timers:\n",
      "    learn_throughput: 1755.435\n",
      "    learn_time_ms: 2278.637\n",
      "    load_throughput: 39624978.744\n",
      "    load_time_ms: 0.101\n",
      "    sample_throughput: 2586.375\n",
      "    sample_time_ms: 1546.566\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1658599352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         72.5299</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  337.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 117</td><td style=\"text-align: right;\">            337.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-40\n",
      "  done: false\n",
      "  episode_len_mean: 353.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 353.22\n",
      "  episode_reward_min: 117.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 741\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5258789062500003e-06\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5318886826435725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0007342701386350035\n",
      "          policy_loss: -0.007886218454849015\n",
      "          total_loss: 1287.9996888519615\n",
      "          vf_explained_var:\n",
      "          - -0.06459195166826248\n",
      "          vf_loss: 1288.0075689500377\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.766666666666666\n",
      "    ram_util_percent: 66.63333333333333\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032694609124964594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.040767668021020595\n",
      "    mean_inference_ms: 0.6347076963214353\n",
      "    mean_raw_obs_processing_ms: 0.04648468297590485\n",
      "  time_since_restore: 80.4921555519104\n",
      "  time_this_iter_s: 4.197028875350952\n",
      "  time_total_s: 80.4921555519104\n",
      "  timers:\n",
      "    learn_throughput: 1735.048\n",
      "    learn_time_ms: 2305.412\n",
      "    load_throughput: 80698489.658\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2560.984\n",
      "    sample_time_ms: 1561.9\n",
      "    update_time_ms: 1.589\n",
      "  timestamp: 1658599360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         80.4922</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  353.22</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 117</td><td style=\"text-align: right;\">            353.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 384.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 384.04\n",
      "  episode_reward_min: 117.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 758\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.814697265625001e-07\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5362070259227547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006244662457598532\n",
      "          policy_loss: -0.007197979413053041\n",
      "          total_loss: 1129.1043415684853\n",
      "          vf_explained_var:\n",
      "          - 0.03205464780330658\n",
      "          vf_loss: 1129.1115376134073\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.933333333333334\n",
      "    ram_util_percent: 67.18333333333332\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03278908163239096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.041077115855659256\n",
      "    mean_inference_ms: 0.6360590256633962\n",
      "    mean_raw_obs_processing_ms: 0.046378566801859096\n",
      "  time_since_restore: 88.96532392501831\n",
      "  time_this_iter_s: 4.12056040763855\n",
      "  time_total_s: 88.96532392501831\n",
      "  timers:\n",
      "    learn_throughput: 1690.052\n",
      "    learn_time_ms: 2366.791\n",
      "    load_throughput: 80698489.658\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2517.441\n",
      "    sample_time_ms: 1588.915\n",
      "    update_time_ms: 1.535\n",
      "  timestamp: 1658599368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         88.9653</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  384.04</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 117</td><td style=\"text-align: right;\">            384.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 406.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 406.34\n",
      "  episode_reward_min: 117.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 777\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.536743164062502e-08\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5370557866109315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017703727857074855\n",
      "          policy_loss: -0.0065581404954515475\n",
      "          total_loss: 1013.0510807488554\n",
      "          vf_explained_var:\n",
      "          - -0.2734878957271576\n",
      "          vf_loss: 1013.0576368106309\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.214285714285715\n",
      "    ram_util_percent: 67.27142857142857\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03300533290513581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04147766963764752\n",
      "    mean_inference_ms: 0.6383992429747564\n",
      "    mean_raw_obs_processing_ms: 0.046333906761783394\n",
      "  time_since_restore: 97.7006893157959\n",
      "  time_this_iter_s: 4.397650718688965\n",
      "  time_total_s: 97.7006893157959\n",
      "  timers:\n",
      "    learn_throughput: 1655.734\n",
      "    learn_time_ms: 2415.847\n",
      "    load_throughput: 40339543.159\n",
      "    load_time_ms: 0.099\n",
      "    sample_throughput: 2448.295\n",
      "    sample_time_ms: 1633.79\n",
      "    update_time_ms: 1.44\n",
      "  timestamp: 1658599377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         97.7007</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  406.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 117</td><td style=\"text-align: right;\">            406.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 422.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 422.34\n",
      "  episode_reward_min: 149.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 793\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3841857910156255e-08\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5294694326898103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006232533626120386\n",
      "          policy_loss: -0.005478058094459195\n",
      "          total_loss: 831.1262467251029\n",
      "          vf_explained_var:\n",
      "          - -0.1372043341398239\n",
      "          vf_loss: 831.1317274442283\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.785714285714285\n",
      "    ram_util_percent: 67.77142857142857\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03324373730809256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042014226674700304\n",
      "    mean_inference_ms: 0.6415758046039515\n",
      "    mean_raw_obs_processing_ms: 0.04644858496006666\n",
      "  time_since_restore: 106.93501830101013\n",
      "  time_this_iter_s: 4.355874538421631\n",
      "  time_total_s: 106.93501830101013\n",
      "  timers:\n",
      "    learn_throughput: 1592.622\n",
      "    learn_time_ms: 2511.582\n",
      "    load_throughput: 80659692.308\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2363.928\n",
      "    sample_time_ms: 1692.099\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1658599386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         106.935</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  422.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 149</td><td style=\"text-align: right;\">            422.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-14\n",
      "  done: false\n",
      "  episode_len_mean: 436.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 436.42\n",
      "  episode_reward_min: 169.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 814\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.960464477539064e-09\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.524110984481791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008735424991992372\n",
      "          policy_loss: -0.0038363754168473266\n",
      "          total_loss: 761.9864349037089\n",
      "          vf_explained_var:\n",
      "          - -0.2087595909833908\n",
      "          vf_loss: 761.9902716359785\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.866666666666664\n",
      "    ram_util_percent: 67.66666666666667\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03354863186095869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04263078954753095\n",
      "    mean_inference_ms: 0.6461685229127557\n",
      "    mean_raw_obs_processing_ms: 0.046660791671381506\n",
      "  time_since_restore: 115.06735134124756\n",
      "  time_this_iter_s: 4.071103811264038\n",
      "  time_total_s: 115.06735134124756\n",
      "  timers:\n",
      "    learn_throughput: 1574.704\n",
      "    learn_time_ms: 2540.16\n",
      "    load_throughput: 80659692.308\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2343.239\n",
      "    sample_time_ms: 1707.039\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1658599394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         115.067</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  436.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 169</td><td style=\"text-align: right;\">            436.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 428.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 428.71\n",
      "  episode_reward_min: 130.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 835\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.490116119384766e-09\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5139690210101425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.000624396397560504\n",
      "          policy_loss: -0.0008645909207482492\n",
      "          total_loss: 742.7697849806918\n",
      "          vf_explained_var:\n",
      "          - -0.2515738606452942\n",
      "          vf_loss: 742.7706527463852\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.549999999999999\n",
      "    ram_util_percent: 67.39999999999999\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03384817352801029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04313664868646608\n",
      "    mean_inference_ms: 0.6506386745706433\n",
      "    mean_raw_obs_processing_ms: 0.046943190372164684\n",
      "  time_since_restore: 122.9899435043335\n",
      "  time_this_iter_s: 3.9034292697906494\n",
      "  time_total_s: 122.9899435043335\n",
      "  timers:\n",
      "    learn_throughput: 1574.153\n",
      "    learn_time_ms: 2541.049\n",
      "    load_throughput: 80659692.308\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2349.812\n",
      "    sample_time_ms: 1702.264\n",
      "    update_time_ms: 1.684\n",
      "  timestamp: 1658599402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">          122.99</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  428.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 130</td><td style=\"text-align: right;\">            428.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 424.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 424.71\n",
      "  episode_reward_min: 110.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 853\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.725290298461915e-10\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5157842105434787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002063405198760814\n",
      "          policy_loss: -0.004870772938073803\n",
      "          total_loss: 514.9122452479537\n",
      "          vf_explained_var:\n",
      "          - -0.10558952391147614\n",
      "          vf_loss: 514.9171175884944\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.9\n",
      "    ram_util_percent: 67.42\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.034070134723248706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04343593684910738\n",
      "    mean_inference_ms: 0.654019448585742\n",
      "    mean_raw_obs_processing_ms: 0.0471568721067583\n",
      "  time_since_restore: 131.13305807113647\n",
      "  time_this_iter_s: 3.969113826751709\n",
      "  time_total_s: 131.13305807113647\n",
      "  timers:\n",
      "    learn_throughput: 1593.645\n",
      "    learn_time_ms: 2509.97\n",
      "    load_throughput: 80659692.308\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2352.603\n",
      "    sample_time_ms: 1700.244\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1658599411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         131.133</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  424.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 110</td><td style=\"text-align: right;\">            424.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 417.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 417.92\n",
      "  episode_reward_min: 110.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 872\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.313225746154787e-11\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5134287555051106\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002227113507155357\n",
      "          policy_loss: -0.003021643873584527\n",
      "          total_loss: 536.1781577981928\n",
      "          vf_explained_var:\n",
      "          - -0.14695963263511658\n",
      "          vf_loss: 536.1811797029229\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.759999999999998\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.034191360796632465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0436911015617101\n",
      "    mean_inference_ms: 0.6566850944147586\n",
      "    mean_raw_obs_processing_ms: 0.04730475947224031\n",
      "  time_since_restore: 139.0263946056366\n",
      "  time_this_iter_s: 3.946547269821167\n",
      "  time_total_s: 139.0263946056366\n",
      "  timers:\n",
      "    learn_throughput: 1622.53\n",
      "    learn_time_ms: 2465.286\n",
      "    load_throughput: 80427689.358\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2408.445\n",
      "    sample_time_ms: 1660.822\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1658599419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         139.026</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  417.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 110</td><td style=\"text-align: right;\">            417.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-47\n",
      "  done: false\n",
      "  episode_len_mean: 417.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 417.03\n",
      "  episode_reward_min: 110.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 890\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3283064365386967e-11\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5037030619639222\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004103962145961061\n",
      "          policy_loss: -0.005808253586292267\n",
      "          total_loss: 509.7463887655607\n",
      "          vf_explained_var:\n",
      "          - -0.25097405910491943\n",
      "          vf_loss: 509.75219681852604\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.48333333333333\n",
      "    ram_util_percent: 67.38333333333334\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03421935097111464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04375592729211263\n",
      "    mean_inference_ms: 0.6577437456632463\n",
      "    mean_raw_obs_processing_ms: 0.04732179524142349\n",
      "  time_since_restore: 147.0986180305481\n",
      "  time_this_iter_s: 4.014173984527588\n",
      "  time_total_s: 147.0986180305481\n",
      "  timers:\n",
      "    learn_throughput: 1673.214\n",
      "    learn_time_ms: 2390.609\n",
      "    load_throughput: 80427689.358\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2469.989\n",
      "    sample_time_ms: 1619.441\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1658599427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         147.099</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">  417.03</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 110</td><td style=\"text-align: right;\">            417.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-03-55\n",
      "  done: false\n",
      "  episode_len_mean: 427.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 427.44\n",
      "  episode_reward_min: 110.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 906\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.820766091346742e-12\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5171368691869961\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001407139072591957\n",
      "          policy_loss: -0.0013227623316549486\n",
      "          total_loss: 461.7417781337615\n",
      "          vf_explained_var:\n",
      "          - -0.055260807275772095\n",
      "          vf_loss: 461.7431020223966\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.8\n",
      "    ram_util_percent: 67.46666666666665\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.034227331099678286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04381136303365791\n",
      "    mean_inference_ms: 0.6583870336130999\n",
      "    mean_raw_obs_processing_ms: 0.047321353112879085\n",
      "  time_since_restore: 155.53555917739868\n",
      "  time_this_iter_s: 4.377202749252319\n",
      "  time_total_s: 155.53555917739868\n",
      "  timers:\n",
      "    learn_throughput: 1659.329\n",
      "    learn_time_ms: 2410.613\n",
      "    load_throughput: 80427689.358\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2453.392\n",
      "    sample_time_ms: 1630.396\n",
      "    update_time_ms: 1.59\n",
      "  timestamp: 1658599435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         155.536</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">  427.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 110</td><td style=\"text-align: right;\">            427.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 435.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 435.13\n",
      "  episode_reward_min: 110.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 923\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4551915228366855e-12\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5195916908402597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0010119088870648354\n",
      "          policy_loss: -4.994449436023671e-05\n",
      "          total_loss: 518.1564171903876\n",
      "          vf_explained_var:\n",
      "          - -0.14341993629932404\n",
      "          vf_loss: 518.1564622981574\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.133333333333333\n",
      "    ram_util_percent: 67.44999999999999\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03431632275919518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04391310840807613\n",
      "    mean_inference_ms: 0.6596143176911764\n",
      "    mean_raw_obs_processing_ms: 0.047376183212626904\n",
      "  time_since_restore: 163.8598780632019\n",
      "  time_this_iter_s: 3.956210136413574\n",
      "  time_total_s: 163.8598780632019\n",
      "  timers:\n",
      "    learn_throughput: 1663.329\n",
      "    learn_time_ms: 2404.816\n",
      "    load_throughput: 80427689.358\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2386.154\n",
      "    sample_time_ms: 1676.338\n",
      "    update_time_ms: 1.54\n",
      "  timestamp: 1658599444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          163.86</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">  435.13</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 110</td><td style=\"text-align: right;\">            435.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-11\n",
      "  done: false\n",
      "  episode_len_mean: 458.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 458.88\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 940\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.6379788070917137e-13\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5094273452476789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0018134001792102453\n",
      "          policy_loss: -0.0013510088446319745\n",
      "          total_loss: 506.42929915561473\n",
      "          vf_explained_var:\n",
      "          - -0.07667318731546402\n",
      "          vf_loss: 506.4306501573132\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.416666666666668\n",
      "    ram_util_percent: 67.39999999999999\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.034375662749432274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04403046677769314\n",
      "    mean_inference_ms: 0.6607373333189451\n",
      "    mean_raw_obs_processing_ms: 0.04743071885933649\n",
      "  time_since_restore: 171.7585220336914\n",
      "  time_this_iter_s: 3.9643585681915283\n",
      "  time_total_s: 171.7585220336914\n",
      "  timers:\n",
      "    learn_throughput: 1674.569\n",
      "    learn_time_ms: 2388.675\n",
      "    load_throughput: 40262097.432\n",
      "    load_time_ms: 0.099\n",
      "    sample_throughput: 2397.984\n",
      "    sample_time_ms: 1668.068\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1658599451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         171.759</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  458.88</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 118</td><td style=\"text-align: right;\">            458.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 455.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 455.31\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 959\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729284e-14\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5135584140977552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0002777872799419324\n",
      "          policy_loss: 0.00201664367310142\n",
      "          total_loss: 577.4896444136097\n",
      "          vf_explained_var:\n",
      "          - -0.049117304384708405\n",
      "          vf_loss: 577.4876301457805\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.92\n",
      "    ram_util_percent: 67.4\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0344972108182862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044120133586499195\n",
      "    mean_inference_ms: 0.6618898268034394\n",
      "    mean_raw_obs_processing_ms: 0.04746729096366917\n",
      "  time_since_restore: 179.6800513267517\n",
      "  time_this_iter_s: 3.914353847503662\n",
      "  time_total_s: 179.6800513267517\n",
      "  timers:\n",
      "    learn_throughput: 1674.414\n",
      "    learn_time_ms: 2388.896\n",
      "    load_throughput: 80620932.244\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2394.142\n",
      "    sample_time_ms: 1670.745\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1658599459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">          179.68</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  455.31</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 118</td><td style=\"text-align: right;\">            455.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-27\n",
      "  done: false\n",
      "  episode_len_mean: 462.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 462.76\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 976\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.273736754432321e-14\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5172271940977343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011645050726921682\n",
      "          policy_loss: -0.0002749759503590163\n",
      "          total_loss: 500.90231264175907\n",
      "          vf_explained_var:\n",
      "          - 0.012865548953413963\n",
      "          vf_loss: 500.9025879234396\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.14\n",
      "    ram_util_percent: 67.3\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03459121452818871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04418215222574561\n",
      "    mean_inference_ms: 0.6628088707086297\n",
      "    mean_raw_obs_processing_ms: 0.047488564168169524\n",
      "  time_since_restore: 187.53354597091675\n",
      "  time_this_iter_s: 3.945826768875122\n",
      "  time_total_s: 187.53354597091675\n",
      "  timers:\n",
      "    learn_throughput: 1683.263\n",
      "    learn_time_ms: 2376.337\n",
      "    load_throughput: 80620932.244\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2407.483\n",
      "    sample_time_ms: 1661.486\n",
      "    update_time_ms: 1.389\n",
      "  timestamp: 1658599467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         187.534</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  462.76</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 118</td><td style=\"text-align: right;\">            462.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-35\n",
      "  done: false\n",
      "  episode_len_mean: 467.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 467.89\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 992\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.6843418860808026e-15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5177878022514364\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0021327557421488277\n",
      "          policy_loss: -0.00020613595843315123\n",
      "          total_loss: 534.5553520325692\n",
      "          vf_explained_var:\n",
      "          - -0.07692240178585052\n",
      "          vf_loss: 534.5555581246653\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.516666666666666\n",
      "    ram_util_percent: 67.38333333333333\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03470656797278964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04419066627226011\n",
      "    mean_inference_ms: 0.6636401437072194\n",
      "    mean_raw_obs_processing_ms: 0.04748987840045398\n",
      "  time_since_restore: 195.60150933265686\n",
      "  time_this_iter_s: 4.075189113616943\n",
      "  time_total_s: 195.60150933265686\n",
      "  timers:\n",
      "    learn_throughput: 1697.564\n",
      "    learn_time_ms: 2356.317\n",
      "    load_throughput: 80620932.244\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2431.846\n",
      "    sample_time_ms: 1644.841\n",
      "    update_time_ms: 1.339\n",
      "  timestamp: 1658599475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         195.602</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  467.89</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 118</td><td style=\"text-align: right;\">            467.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 460.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 460.6\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1010\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4210854715202006e-15\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5168829057806281\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0014635509966831955\n",
      "          policy_loss: -0.0008508720825756749\n",
      "          total_loss: 590.7483871788107\n",
      "          vf_explained_var:\n",
      "          - -0.16594508290290833\n",
      "          vf_loss: 590.7492373517764\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.23333333333333\n",
      "    ram_util_percent: 67.6\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03477882601226384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04421492625186506\n",
      "    mean_inference_ms: 0.6641711833479254\n",
      "    mean_raw_obs_processing_ms: 0.047454099652062764\n",
      "  time_since_restore: 204.00663232803345\n",
      "  time_this_iter_s: 4.279141426086426\n",
      "  time_total_s: 204.00663232803345\n",
      "  timers:\n",
      "    learn_throughput: 1670.245\n",
      "    learn_time_ms: 2394.858\n",
      "    load_throughput: 80620932.244\n",
      "    load_time_ms: 0.05\n",
      "    sample_throughput: 2477.721\n",
      "    sample_time_ms: 1614.387\n",
      "    update_time_ms: 1.241\n",
      "  timestamp: 1658599484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         204.007</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">   460.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 118</td><td style=\"text-align: right;\">             460.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-04-52\n",
      "  done: false\n",
      "  episode_len_mean: 466.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 466.29\n",
      "  episode_reward_min: 118.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1026\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.5527136788005016e-16\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5320044406639632\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0005547467038799562\n",
      "          policy_loss: 9.582854046296048e-05\n",
      "          total_loss: 529.6212682949599\n",
      "          vf_explained_var:\n",
      "          - -0.049634259194135666\n",
      "          vf_loss: 529.6211717421008\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.25\n",
      "    ram_util_percent: 67.7\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03485057212795117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044225115921889185\n",
      "    mean_inference_ms: 0.6645418290827443\n",
      "    mean_raw_obs_processing_ms: 0.04742986980667025\n",
      "  time_since_restore: 212.48624181747437\n",
      "  time_this_iter_s: 4.071178674697876\n",
      "  time_total_s: 212.48624181747437\n",
      "  timers:\n",
      "    learn_throughput: 1653.175\n",
      "    learn_time_ms: 2419.586\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2427.498\n",
      "    sample_time_ms: 1647.787\n",
      "    update_time_ms: 1.291\n",
      "  timestamp: 1658599492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         212.486</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  466.29</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 118</td><td style=\"text-align: right;\">            466.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-05-01\n",
      "  done: false\n",
      "  episode_len_mean: 471.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 471.88\n",
      "  episode_reward_min: 175.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1042\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.881784197001254e-17\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5269133135836611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002458919508846222\n",
      "          policy_loss: -0.00151736976638917\n",
      "          total_loss: 499.00465960348805\n",
      "          vf_explained_var:\n",
      "          - -0.011730234138667583\n",
      "          vf_loss: 499.0061767126924\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.700000000000003\n",
      "    ram_util_percent: 67.56666666666666\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03492085400208607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0442670249630336\n",
      "    mean_inference_ms: 0.6649657011950981\n",
      "    mean_raw_obs_processing_ms: 0.04742573211004224\n",
      "  time_since_restore: 220.62596988677979\n",
      "  time_this_iter_s: 4.04979944229126\n",
      "  time_total_s: 220.62596988677979\n",
      "  timers:\n",
      "    learn_throughput: 1642.851\n",
      "    learn_time_ms: 2434.791\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2417.954\n",
      "    sample_time_ms: 1654.291\n",
      "    update_time_ms: 1.29\n",
      "  timestamp: 1658599501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         220.626</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  471.88</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 175</td><td style=\"text-align: right;\">            471.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_6e22b_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-07-23_20-05-09\n",
      "  done: true\n",
      "  episode_len_mean: 475.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 475.73\n",
      "  episode_reward_min: 182.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1059\n",
      "  experiment_id: abe6ee00f7dd492fb81d51c8d37b7746\n",
      "  hostname: LAPTOP-MUL4L8MS\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2204460492503135e-17\n",
      "          cur_lr: 5.0000000000000016e-05\n",
      "          entropy: 0.5208867065047705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0002004759106740692\n",
      "          policy_loss: -0.0009501318677618939\n",
      "          total_loss: 382.68431322651526\n",
      "          vf_explained_var:\n",
      "          - -0.035070717334747314\n",
      "          vf_loss: 382.6852656948951\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.12\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.05\n",
      "    ram_util_percent: 67.41666666666667\n",
      "  pid: 7416\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03498626441085847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04429406488980295\n",
      "    mean_inference_ms: 0.6653982698230676\n",
      "    mean_raw_obs_processing_ms: 0.04743659011516776\n",
      "  time_since_restore: 228.56157183647156\n",
      "  time_this_iter_s: 3.939235210418701\n",
      "  time_total_s: 228.56157183647156\n",
      "  timers:\n",
      "    learn_throughput: 1640.828\n",
      "    learn_time_ms: 2437.794\n",
      "    load_throughput: 0.0\n",
      "    load_time_ms: 0.0\n",
      "    sample_throughput: 2410.414\n",
      "    sample_time_ms: 1659.466\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1658599509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6e22b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>RUNNING </td><td>192.168.1.12:7416</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         228.562</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  475.73</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            475.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/3.81 GiB heap, 0.0/1.9 GiB objects<br>Result logdir: C:\\Users\\brieg\\ray_results\\PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_6e22b_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         228.562</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  475.73</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 182</td><td style=\"text-align: right;\">            475.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 20:05:09,773\tINFO tune.py:561 -- Total run time: 240.72 seconds (240.05 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    # “tf” to use tensorflow, \"torch\" to use pytorch\n",
    "    \"framework\": \"torch\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [32],\n",
    "        \"fcnet_activation\": \"linear\",\n",
    "    },\n",
    "}\n",
    "stop = {\"episode_reward_mean\": 475}\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=4, include_dashboard=False,\n",
    "         ignore_reinit_error=True, log_to_driver=False)\n",
    "# Start Training \n",
    "analysis = ray.tune.run(\"PPO\", config=config,\n",
    "                        stop=stop, checkpoint_at_end=True, fail_fast=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91148200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 20:05:48,772\tINFO ppo.py:158 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-07-23 20:05:48,773\tINFO trainer.py:726 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-07-23 20:05:53,709\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-07-23 20:05:53,733\tINFO trainable.py:382 -- Restored on 192.168.1.12 from checkpoint: C:\\Users\\brieg\\ray_results\\PPO\\PPO_CartPole-v1_6e22b_00000_0_2022-07-23_20-01-09\\checkpoint_000057\\checkpoint-57\n",
      "2022-07-23 20:05:53,734\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 57, '_timesteps_total': None, '_time_total': 228.56157183647156, '_episodes_total': 1059}\n"
     ]
    }
   ],
   "source": [
    "# restore a trainer from the last checkpoint\n",
    "trial = analysis.get_best_logdir(\"episode_reward_mean\", \"max\")\n",
    "checkpoint = analysis.get_best_checkpoint(\n",
    "  trial,\n",
    "  \"training_iteration\",\n",
    "  \"max\",\n",
    ")\n",
    "trainer = PPOTrainer(config=config)\n",
    "trainer.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fa1c3",
   "metadata": {},
   "source": [
    "## Evaluate the Agent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec54623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 20:12:20,268\tWARNING worker.py:1215 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff194a43b0f15001e46eeda6c201000000 Worker ID: dca91bdcff2fe9653d119fab737fb3470745531b97baae2e7da3e129 Node ID: 28d73e49d54952664dd89d68adbf4158a9ae57c22075b6632d16b2e8 Worker IP address: 192.168.1.12 Worker port: 56309 Worker PID: 11120\n",
      "2022-07-23 20:12:20,270\tWARNING worker.py:1215 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffb66fa03591f9a4c8a2fb80fb01000000 Worker ID: 45fd5ff7bc336ca3a26cff6288dc2e85e8bf5c6118e805c4da908593 Node ID: 28d73e49d54952664dd89d68adbf4158a9ae57c22075b6632d16b2e8 Worker IP address: 192.168.1.12 Worker port: 50332 Worker PID: 17472\n"
     ]
    }
   ],
   "source": [
    "video_name = \"after_training\"\n",
    "video_path = check_video_folder_sanity(path, video_name)\n",
    "\n",
    "after_video = VideoRecorder(env, video_path + \".mp4\", enabled=video_name is not None)\n",
    "observation = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    after_video.capture_frame()\n",
    "    action = trainer.compute_single_action(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "after_video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "512167d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/cart_pole/after_training.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_path + \".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f9c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
